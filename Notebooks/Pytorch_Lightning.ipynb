{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0133fec1-13b1-47bb-be64-e455ab94f663",
   "metadata": {},
   "source": [
    "## Using PyTorch Lightning\n",
    "\n",
    "### by Michael Ruddy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e1bea-335e-413a-8cb7-ee6a13a6bd15",
   "metadata": {},
   "source": [
    "To get PyTorch Lightning:\n",
    "\n",
    "`conda install -c conda-forge pytorch-lightning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3269a8b5-00f0-48f1-9de1-00b419962ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PyTorch stuff\n",
    "import torch, torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e707ab-a687-49bb-8fd9-8193f1fc4f76",
   "metadata": {},
   "source": [
    "Let's use the MNIST dataset to test out these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c6853-9675-4c64-a8fa-06234b00811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the MNIST dataset\n",
    "trnsfm = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((.5), (.5))])\n",
    "\n",
    "ds_train = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=trnsfm)\n",
    "ds_val = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=trnsfm)\n",
    "\n",
    "# I'm going to do more than one \"run\" in this notebook\n",
    "global_hyperparam = {'N_train':len(ds_train),\n",
    "                     'N_val':len(ds_val)}\n",
    "\n",
    "batch_size = 4\n",
    "global_hyperparam['batch_size'] = batch_size\n",
    "\n",
    "# dataloaders\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d5519-c5b2-46a4-85ae-e9da0203f7ab",
   "metadata": {},
   "source": [
    "### PyTorch Set-Up + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157d933-fd28-474f-8523-5bbc24fd1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class small_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "                \n",
    "        self.linear1 = nn.Linear(64*7*7, 100)\n",
    "        self.linear2 = nn.Linear(100, 10)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.unroll = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.linear1(self.unroll(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f82a43d-d2f2-4a40-9d07-c3bd550c11b0",
   "metadata": {},
   "source": [
    "Note that these next functions must be altered if I alter the task of number of inputs to the model forward pass or if I want to switch to regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e479948-c7dc-4ccf-af00-317e6b99e37d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one pass through the dataloader, keyword for whether to backprop or not\n",
    "def one_pass(model, dataloader, optimizer, scheduler, lossFun, backwards=True, print_loss=False):\n",
    "    \n",
    "    if backwards == True:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for x, y in tqdm(dataloader):\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = lossFun(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if backwards == True:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    if print_loss == True:\n",
    "        print(avg_loss)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "# one pass to gather metrics\n",
    "def one_pass_acc(model, dataloader, num_points):\n",
    "    model.eval()\n",
    "    total_incorrect = 0\n",
    "    \n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        y_pred = softmax(model(x))\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "        total_incorrect += torch.count_nonzero(y - y_pred).item()\n",
    "        \n",
    "    acc = 1 - (total_incorrect / num_points)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ccf50-2392-4651-8ad4-58700d98e9f3",
   "metadata": {},
   "source": [
    "The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fddb2e5-9b2e-482b-81de-780070b18704",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "model = small_CNN()\n",
    "lossFun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002, epochs=num_epochs, steps_per_epoch=len(dl_train))\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    train_loss = one_pass(model, dl_train, optimizer, lr_scheduler, lossFun)\n",
    "    valid_loss = one_pass(model, dl_val, optimizer, lr_scheduler, lossFun, backwards=False)\n",
    "    \n",
    "    print(f\"Train loss, Epoch {epoch}:\", train_loss)\n",
    "    print(f\"Val loss, Epoch {epoch}:\", valid_loss)\n",
    "    \n",
    "    train_acc = one_pass_acc(model, dl_train, len(ds_train))\n",
    "    valid_acc = one_pass_acc(model, dl_val, len(ds_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144760a-78a1-469c-9414-8f715e469cd1",
   "metadata": {},
   "source": [
    "Now let's do the same model and training in PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e837b-0839-4394-b696-8f6a06e9169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lightning_small_CNN(pl.LightningModule):\n",
    "    \n",
    "    # Similarly need to set-up the weights and the forward pass\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "                \n",
    "        self.linear1 = nn.Linear(64*7*7, 100)\n",
    "        self.linear2 = nn.Linear(100, 10)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.unroll = nn.Flatten()\n",
    "        \n",
    "        # going to attach the loss function to the module\n",
    "        self.CELoss = nn.CrossEntropyLoss()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        # need for scheduler, can't named self.hparams\n",
    "        self.hp = hparams\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.linear1(self.unroll(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    # method for computing the loss\n",
    "    def lossFun(self, y_pred, y):\n",
    "        return self.CELoss(y_pred, y)\n",
    "    \n",
    "    # we can define our metric functions below\n",
    "    def acc(self, y_pred, y):\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "        total_incorrect = torch.count_nonzero(y - y_pred).item()\n",
    "        \n",
    "        return 1 - (total_incorrect / torch.numel(y))\n",
    "    \n",
    "    # this method must be named training_step\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        \n",
    "        x, y = train_batch\n",
    "\n",
    "        # now these functions are wrapped up in self\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.lossFun(y_pred, y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        \n",
    "        # compute metrics\n",
    "        acc = self.acc(y_pred, y)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # instead of a on/off switch for the backward pass, we simply define a separate step for validation\n",
    "    # must be named validation_step\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.lossFun(y_pred, y)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        acc = self.acc(y_pred, y)\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    # here we configure the optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        \n",
    "        # we can even pass the scheduler!\n",
    "        # because the annealing scheduler needs to know the number of batches and epochs, we'll pass a hparam dictionary to the model later\n",
    "        lr_scheduler = {\n",
    "                        'scheduler': optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002,\n",
    "                                                     epochs=self.hp['num_epochs'],\n",
    "                                                     steps_per_epoch=self.hp['num_batches']),\n",
    "                        'interval': 'step'  # forces updates after each training step, instead of per epoch\n",
    "                        }\n",
    "        # we pass lists here, because lightning support multiple optimizers!\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6bffe6-8e09-476e-9a5c-9e566b6ee663",
   "metadata": {},
   "source": [
    "Now we have the same training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf21a86-be81-4062-abef-7248c652d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "hparams = {'num_epochs': num_epochs,\n",
    "           'num_batches': len(dl_train)}\n",
    "model = lightning_small_CNN(hparams)\n",
    "trainer = pl.Trainer(max_epochs=num_epochs)\n",
    "\n",
    "trainer.fit(model, dl_train, dl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7637d-4c13-4857-a488-7672d990bbed",
   "metadata": {},
   "source": [
    "There's a few nice bells and whistles here.\n",
    "- Automatic progress bar!\n",
    "- Makes logging the train and validation loss easy (logs stored in lightning_logs\n",
    "- Trainer first makes sure the forward loop runs on the validation set\n",
    "- Most of the training loop can be abstracted to the Module which makes training from scripts very easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe332d3-1b85-4522-8a0a-a6755adb00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up tensorboard to view the logs!\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d1e6c-8b8a-4590-80a0-f11fae57ba6c",
   "metadata": {},
   "source": [
    "We can even pair with neptune.ai, need to run the following first:\n",
    "\n",
    "`pip install neptune-pytorch-lightning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84f04d-ecaa-4748-9b06-d94df46a8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.new.integrations.pytorch_lightning import NeptuneLogger\n",
    "\n",
    "# frustratingly enough note that api_token is called api_key here!\n",
    "run = NeptuneLogger(\n",
    "    project=\"your_project_name\",\n",
    "    api_key=\"your_api_key\",\n",
    "    name = \"Lightning_Test\",\n",
    ")\n",
    "\n",
    "num_epochs = 2\n",
    "hparams = {'num_epochs': num_epochs,\n",
    "           'num_batches': len(dl_train)}\n",
    "model = lightning_small_CNN(hparams)\n",
    "trainer = pl.Trainer(max_epochs=num_epochs, logger=run)\n",
    "\n",
    "trainer.fit(model, dl_train, dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1215ff-5a71-48bd-a71e-125b850c75b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
