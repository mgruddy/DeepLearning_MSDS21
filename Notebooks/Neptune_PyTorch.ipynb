{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500564d8-f85b-43b9-bff1-90c0bd51b941",
   "metadata": {},
   "source": [
    "## Using neptune.ai with PyTorch to log information during model development\n",
    "\n",
    "### by Michael Ruddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f7b10-9d68-48e3-9286-12e0a0a63fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PyTorch stuff\n",
    "import torch, torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Neptune\n",
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a01b0-fc0e-4eae-9b58-ee327ba1c176",
   "metadata": {},
   "source": [
    "Let's use the MNIST dataset to test out these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f0b673-95b0-499b-92f3-6652dcc96ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the MNIST dataset\n",
    "trnsfm = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((.5), (.5))])\n",
    "\n",
    "ds_train = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=trnsfm)\n",
    "ds_val = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=trnsfm)\n",
    "\n",
    "# I'm going to do more than one \"run\" in this notebook\n",
    "global_hyperparam = {'N_train':len(ds_train),\n",
    "                     'N_val':len(ds_val)}\n",
    "\n",
    "batch_size = 4\n",
    "global_hyperparam['batch_size'] = batch_size\n",
    "\n",
    "# dataloaders\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val),\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e33878-00bc-47e2-a1f8-7e26e13817e8",
   "metadata": {},
   "source": [
    "And a very simple CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e41cc-9d6b-4274-9ba9-1564055c8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple CNN\n",
    "class small_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "                \n",
    "        self.linear1 = nn.Linear(64*7*7, 100)\n",
    "        self.linear2 = nn.Linear(100, 10)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.unroll = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.linear1(self.unroll(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214377f6-6e06-4ee6-97ed-5a82ed128389",
   "metadata": {},
   "source": [
    "Here we have some simple functions to train epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c982a64-2cf9-41b9-bcb3-d5742ac07e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_pass(model, dataloader, optimizer, lossFun, backwards=True, print_loss=False, log=None):\n",
    "    \n",
    "    if backwards == True:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for x, y in tqdm(dataloader):\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = lossFun(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # pass the key name to log the loss each batch\n",
    "        if log:\n",
    "            run[log].log(loss.item())\n",
    "        \n",
    "        if backwards == True:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    if print_loss == True:\n",
    "        print(avg_loss)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def one_pass_acc(model, dataloader, num_points):\n",
    "    model.eval()\n",
    "    total_incorrect = 0\n",
    "    \n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        y_pred = softmax(model(x))\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "        total_incorrect += torch.count_nonzero(y - y_pred).item()\n",
    "        \n",
    "    acc = 1 - (total_incorrect / num_points)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58ad52-c79b-46dc-823b-1603f54b4828",
   "metadata": {},
   "source": [
    "Now let's perform an experiment. We must first create a project using our account at neptune.ai and get the api_token and project name from there. We'll keep track of various hyperparameters, but also statistics about training such as the training/validation loss each epoch. Finally we can save the model parameters as well.\n",
    "\n",
    "Some helpful tidbits:\n",
    "- The choice of organizing the set-up into a `config` folder is arbitrary. I can organize this information however I please. Same thing with train and validation folders.\n",
    "- What is helpful to make sure that these have the same organization across runs to make comparison easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbde3ce-91cb-4fc8-8d12-e43cfa82bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a run\n",
    "run = neptune.init(\n",
    "    project=\"your_project_name\",\n",
    "    api_token=\"your_api_key\",\n",
    "    name = \"Small_CNN\",\n",
    "    tags = [\"Scratch\", \"3 Downsamples\"]\n",
    ")\n",
    "\n",
    "# set up model and training\n",
    "model = small_CNN()\n",
    "lossFun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# log the set-up\n",
    "for key, value in global_hyperparam.items():\n",
    "    run[f'config/{key}'] = value\n",
    "    \n",
    "run['config/model'] = type(model).__name__\n",
    "run['config/criterion'] = type(lossFun).__name__\n",
    "run['config/optimizer'] = type(optimizer).__name__\n",
    "run['config/params'] = {\"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                        \"epoch_nr\" : num_epochs}\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    train_loss = one_pass(model, dl_train, optimizer, lossFun, log=\"train/batch_loss\")\n",
    "    valid_loss = one_pass(model, dl_val, optimizer, lossFun, backwards=False)\n",
    "    \n",
    "    train_acc = one_pass_acc(model, dl_train, len(ds_train))\n",
    "    valid_acc = one_pass_acc(model, dl_val, len(ds_val))\n",
    "    \n",
    "    # log the loss and accuracy each epoch\n",
    "    run[\"train/loss\"].log(train_loss)\n",
    "    run[\"val/loss\"].log(valid_loss)\n",
    "    run[\"train/acc\"].log(train_acc)\n",
    "    run[\"val/acc\"].log(valid_acc)\n",
    "\n",
    "# save your progress\n",
    "checkpoint = {'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict' :optimizer.state_dict()}\n",
    "torch.save(checkpoint, 'model_checkpoint.pt')\n",
    "\n",
    "# upload the model weights along with an architecture description\n",
    "run['model/model_checkpoint'].upload('model_checkpoint.pt')\n",
    "\n",
    "# save model architecture description\n",
    "model_arch = open(\"model_arch.txt\", \"w\")\n",
    "model_arch.write(str(model))\n",
    "model_arch.close()\n",
    "run['model/architecture'].upload(\"model_arch.txt\")\n",
    "    \n",
    "# stop logging this run\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e38bd58-32b1-4edf-bb1e-a8b11412988f",
   "metadata": {},
   "source": [
    "Let's say I close the notebook and want to go back and keep logging the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98444bb6-4040-4317-a765-05012c201271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get back to that same run\n",
    "run = neptune.init(\n",
    "    project=\"your_project_name\",\n",
    "    api_token=\"your_api_key\",\n",
    "    run='NEP-1'\n",
    ")\n",
    "\n",
    "# downloads the file with the same name (will overwrite if already there)\n",
    "run['model/model_checkpoint'].download()\n",
    "\n",
    "# set up model and training again\n",
    "model = small_CNN()\n",
    "lossFun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# load up the previous checkpoint\n",
    "# model architecture must be the same!\n",
    "checkpoint = torch.load('model_checkpoint.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "for key, value in global_hyperparam.items():\n",
    "    run[f'config/{key}'] = value\n",
    "    \n",
    "run['config/model'] = type(model).__name__\n",
    "run['config/criterion'] = type(lossFun).__name__\n",
    "run['config/optimizer'] = type(optimizer).__name__\n",
    "run['config/params'] = {\"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                        \"epoch_nr\" : num_epochs}\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    train_loss = one_pass(model, dl_train, optimizer, lossFun, log=\"train/batch_loss\")\n",
    "    valid_loss = one_pass(model, dl_val, optimizer, lossFun, backwards=False)\n",
    "    \n",
    "    train_acc = one_pass_acc(model, dl_train, len(ds_train))\n",
    "    valid_acc = one_pass_acc(model, dl_val, len(ds_val))\n",
    "    \n",
    "    # continue to log the loss and accuracy each epoch\n",
    "    run[\"train/loss\"].log(train_loss)\n",
    "    run[\"val/loss\"].log(valid_loss)\n",
    "    run[\"train/acc\"].log(train_acc)\n",
    "    run[\"val/acc\"].log(valid_acc)\n",
    "\n",
    "# save your progress again\n",
    "checkpoint = {'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict' :optimizer.state_dict()}\n",
    "torch.save(checkpoint, 'model_checkpoint.pt')\n",
    "run['model/model_checkpoint'].upload('model_checkpoint.pt')\n",
    "    \n",
    "# stop logging the run\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f45c01-bf27-4f53-82a4-ff76fcc9a92c",
   "metadata": {},
   "source": [
    "Let's compare to a different style of model. After running this, go to the Compare Runs tab in neptune.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f61ca-5f9a-4f76-bae3-7a881020f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple CNN\n",
    "class smaller_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "                \n",
    "        self.linear1 = nn.Linear(32*14*14, 100)\n",
    "        self.linear2 = nn.Linear(100, 10)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.unroll = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.linear1(self.unroll(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a0d9c-f603-4db0-87ea-0469b2703042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a run\n",
    "run = neptune.init(\n",
    "    project=\"your_project_name\",\n",
    "    api_token=\"your_api_key\",\n",
    "    name = \"Smaller_CNN\",\n",
    "    tags = [\"Scratch\", \"2 Downsamples\"]\n",
    ")\n",
    "\n",
    "# set up model and training\n",
    "model = smaller_CNN()\n",
    "lossFun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# log the set-up\n",
    "for key, value in global_hyperparam.items():\n",
    "    run[f'config/{key}'] = value\n",
    "    \n",
    "run['config/model'] = type(model).__name__\n",
    "run['config/criterion'] = type(lossFun).__name__\n",
    "run['config/optimizer'] = type(optimizer).__name__\n",
    "run['config/params'] = {\"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                        \"epoch_nr\" : num_epochs}\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    train_loss = one_pass(model, dl_train, optimizer, lossFun, log=\"train/batch_loss\")\n",
    "    valid_loss = one_pass(model, dl_val, optimizer, lossFun, backwards=False)\n",
    "    \n",
    "    train_acc = one_pass_acc(model, dl_train, len(ds_train))\n",
    "    valid_acc = one_pass_acc(model, dl_val, len(ds_val))\n",
    "    \n",
    "    # log the loss and accuracy each epoch\n",
    "    run[\"train/loss\"].log(train_loss)\n",
    "    run[\"val/loss\"].log(valid_loss)\n",
    "    run[\"train/acc\"].log(train_acc)\n",
    "    run[\"val/acc\"].log(valid_acc)\n",
    "\n",
    "# save your progress\n",
    "checkpoint = {'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict' :optimizer.state_dict()}\n",
    "torch.save(checkpoint, 'model_checkpoint.pt')\n",
    "\n",
    "# upload the model weights along with an architecture description\n",
    "run['model/model_checkpoint'].upload('model_checkpoint.pt')\n",
    "\n",
    "# save model architecture description\n",
    "model_arch = open(\"model_arch.txt\", \"w\")\n",
    "model_arch.write(str(model))\n",
    "model_arch.close()\n",
    "run['model/architecture'].upload(\"model_arch.txt\")\n",
    "    \n",
    "# stop logging this run\n",
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
